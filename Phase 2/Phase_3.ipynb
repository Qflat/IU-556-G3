{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab902a14-5338-4f47-ab24-4f27244cad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(),'..') # This line may need to be adjusted depending on where the csv files are stored on your local env\n",
    "application = pd.read_csv(os.path.join(data_dir,\"application_train.csv\"))\n",
    "bureau = pd.read_csv(os.path.join(data_dir,\"bureau.csv\"))\n",
    "bureau_balance = pd.read_csv(os.path.join(data_dir,\"bureau_balance.csv\"))\n",
    "previous_application = pd.read_csv(os.path.join(data_dir,\"previous_application.csv\"))\n",
    "POS_CASH_balance = pd.read_csv(os.path.join(data_dir,\"POS_CASH_balance.csv\"))\n",
    "installments_payments = pd.read_csv(os.path.join(data_dir,\"installments_payments.csv\"))\n",
    "credit_card_balance = pd.read_csv(os.path.join(data_dir,\"credit_card_balance.csv\"))\n",
    "app_train = pd.read_csv(os.path.join(data_dir,\"application_train.csv\"))\n",
    "\n",
    "y = application[\"TARGET\"]\n",
    "X = application.drop(\"TARGET\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e42ca2-eca0-4ef8-97df-866667a6df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to select numerical or categorical columns\n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "# Identify the numeric features we wish to consider.\n",
    "num_attribs = [\n",
    "    'AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2','EXT_SOURCE_3']\n",
    " \n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "# Identify the categorical features we wish to consider.\n",
    "cat_attribs = ['CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE',\n",
    "               'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
    " \n",
    "# Notice handle_unknown=\"ignore\" in OHE which ignore values from the validation/test that\n",
    "# do NOT occur in the training set\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        #('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "        ('ohe', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "    ])\n",
    " \n",
    "data_prep_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bbce82-a33a-450d-be51-1af0b7625d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1801\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080794 -> initscore=-2.431606\n",
      "[LightGBM] [Info] Start training from score -2.431606\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model Pipeline\n",
    "#%%time\n",
    "np.random.seed(42)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_prep_pipeline),\n",
    "        (\"logistic\", LogisticRegression(penalty='l2', class_weight='balanced', random_state=42,C=0.1,n_jobs=-1,solver='liblinear')\n",
    "        )\n",
    "    ])\n",
    "model_logistic = full_pipeline_with_predictor.fit(X_train, y_train)\n",
    " \n",
    "# Random Forest Model Pipeline\n",
    "#%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(42)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_prep_pipeline),\n",
    "        (\"random_forest\", RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "model_rf = full_pipeline_with_predictor.fit(X_train, y_train)\n",
    " # XGBoost Model Pipeline\n",
    "#%pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    " \n",
    " \n",
    "np.random.seed(42)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_prep_pipeline),\n",
    "        (\"xgb\", XGBClassifier(random_state=42,\n",
    "                              use_label_encoder=False,\n",
    "                              eval_metric='logloss',scale_pos_weight=11.47))\n",
    "    ])\n",
    "model_xgb = full_pipeline_with_predictor.fit(X_train, y_train)\n",
    " \n",
    "# LightGBM Model Pipeline\n",
    "#%pip install lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    " \n",
    " \n",
    "#%%time\n",
    "np.random.seed(42)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_prep_pipeline),\n",
    "        (\"lgbm\", LGBMClassifier(scale_pos_weight=11.47,random_state=42)) # Scale weight set becasue of class imbalance\n",
    "    ])\n",
    "model_lgbm = full_pipeline_with_predictor.fit(X_train, y_train)\n",
    " \n",
    "# CatBoost Model Pipeline\n",
    "#%pip install catboost\n",
    "import catboost\n",
    " \n",
    "#%%time\n",
    " \n",
    "np.random.seed(42)\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_prep_pipeline),\n",
    "        (\"catboost\", catboost.CatBoostClassifier(verbose=0,\n",
    "        auto_class_weights='Balanced'))\n",
    "    ])\n",
    "model_cat = full_pipeline_with_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a1657e-bcff-416f-b0c0-79ee761f2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dataset(df, group_col, prefix, agg_dict):\n",
    "    \"\"\"Aggregates a dataframe by group_col using agg_dict and renames columns.\"\"\"\n",
    "    agg_df = df.groupby(group_col).agg(agg_dict)\n",
    "    agg_df.columns = [f\"{prefix}_{col}_{func}\".replace(\" \", \"_\") \n",
    "                      for col, funcs in agg_dict.items() for func in funcs]\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "# --- Aggregate Bureau Balance first (if available) ---\n",
    "if 'bureau_balance' in globals() or 'bureau_balance' in locals():\n",
    "    bb_agg = aggregate_dataset(\n",
    "        bureau_balance,\n",
    "        group_col='SK_ID_BUREAU',\n",
    "        prefix='BB',\n",
    "        agg_dict={\n",
    "            'MONTHS_BALANCE': ['min', 'max', 'mean'],\n",
    "            'STATUS': ['nunique']\n",
    "        }\n",
    "    )\n",
    "    bureau = bureau.merge(bb_agg, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "# --- Bureau aggregate by SK_ID_CURR ---\n",
    "bureau_agg = aggregate_dataset(\n",
    "    bureau,\n",
    "    group_col='SK_ID_CURR',\n",
    "    prefix='BUREAU',\n",
    "    agg_dict={\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'std'],\n",
    "        'AMT_CREDIT_SUM': ['sum', 'mean', 'max'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['sum', 'mean'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['sum', 'max'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max'],\n",
    "        'SK_ID_BUREAU': ['count'],  # number of credit lines\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ratio features inside bureau\n",
    "bureau_agg['BUREAU_DEBT_RATIO'] = (\n",
    "    bureau_agg['BUREAU_AMT_CREDIT_SUM_DEBT_sum'] /\n",
    "    bureau_agg['BUREAU_AMT_CREDIT_SUM_sum']\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "prev_agg = aggregate_dataset(\n",
    "    previous_application,\n",
    "    group_col='SK_ID_CURR',\n",
    "    prefix='PREV',\n",
    "    agg_dict={\n",
    "        'AMT_APPLICATION': ['mean', 'min', 'max'],\n",
    "        'AMT_CREDIT': ['mean', 'min', 'max'],\n",
    "        'AMT_DOWN_PAYMENT': ['mean', 'min', 'max'],\n",
    "        'DAYS_FIRST_DRAWING': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['mean'],\n",
    "        'NAME_CONTRACT_STATUS': ['nunique'],\n",
    "        'SK_ID_PREV': ['count']  # number of previous apps\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simple derived features\n",
    "prev_agg['PREV_APPLICATION_CREDIT_DIFF_mean'] = \\\n",
    "    prev_agg['PREV_AMT_APPLICATION_mean'] - prev_agg['PREV_AMT_CREDIT_mean']\n",
    "\n",
    "prev_agg['PREV_APPLICATION_CREDIT_RATIO_mean'] = \\\n",
    "    prev_agg['PREV_AMT_APPLICATION_mean'] / prev_agg['PREV_AMT_CREDIT_mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your main data is called: application\n",
    "application = application.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "application = application.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Fill NaNs where needed (optional)\n",
    "application.fillna(0, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
